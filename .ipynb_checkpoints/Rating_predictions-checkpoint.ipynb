{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d7d309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:12:06.179159Z",
     "iopub.status.busy": "2023-03-28T00:12:06.178164Z",
     "iopub.status.idle": "2023-03-28T00:12:15.802834Z",
     "shell.execute_reply": "2023-03-28T00:12:15.801835Z",
     "shell.execute_reply.started": "2023-03-28T00:12:06.179159Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trevor.sauerbrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\trevor.sauerbrey\\WPy64-3890\\python-3.8.9.amd64\\lib\\site-packages\\xgboost\\compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing, math and plotting\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import stats \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, auc, classification_report, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c1b3f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:12:15.805837Z",
     "iopub.status.busy": "2023-03-28T00:12:15.804834Z",
     "iopub.status.idle": "2023-03-28T00:12:16.350832Z",
     "shell.execute_reply": "2023-03-28T00:12:16.349832Z",
     "shell.execute_reply.started": "2023-03-28T00:12:15.804834Z"
    }
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "RANDOM_STATE = 12\n",
    "TRAIN_SPLIT = 0.80\n",
    "OOV_TOK = \"<OOV>\"\n",
    "VOCAB_SIZE = 10000\n",
    "EMBEDDING_DIM = 16\n",
    "MAX_LENGTH = 120\n",
    "BATCH_SIZE=128\n",
    "EPOCHS=25\n",
    "EARLY_STOPPING_CRITERIA=3\n",
    "DROPOUT_P=0.4\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM =0.9\n",
    "MAX_ITER = 10000\n",
    "N_JOBS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ae668b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:18:03.220277Z",
     "iopub.status.busy": "2023-03-28T00:18:03.219275Z",
     "iopub.status.idle": "2023-03-28T00:18:04.136272Z",
     "shell.execute_reply": "2023-03-28T00:18:04.135274Z",
     "shell.execute_reply.started": "2023-03-28T00:18:03.220277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id                      0\n",
      "order_id                       0\n",
      "review_score                   0\n",
      "review_comment_title       87656\n",
      "review_comment_message     58247\n",
      "review_creation_date           0\n",
      "review_answer_timestamp        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_comment_title</th>\n",
       "      <th>review_comment_message</th>\n",
       "      <th>review_creation_date</th>\n",
       "      <th>review_answer_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7bc2406110b926393aa56f80a40eba40</td>\n",
       "      <td>73fc7af87114b39712e6da79b0a377eb</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-18 00:00:00</td>\n",
       "      <td>2018-01-18 21:46:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80e641a11e56f04c1ad469d5645fdfde</td>\n",
       "      <td>a548910a1c6147796b98fdf73dbeba33</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-10 00:00:00</td>\n",
       "      <td>2018-03-11 03:05:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228ce5500dc1d8e020d8d1322874b6f0</td>\n",
       "      <td>f9e4b658b201a9f2ecdecbb34bed034b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-17 00:00:00</td>\n",
       "      <td>2018-02-18 14:36:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e64fb393e7b32834bb789ff8bb30750e</td>\n",
       "      <td>658677c97b385a9be170737859d3511b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "      <td>2017-04-21 00:00:00</td>\n",
       "      <td>2017-04-21 22:02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f7c4243c7fe1938f181bec41a392bdeb</td>\n",
       "      <td>8e6bfb81e283fa7e4f11123a3fb894f1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parabéns lojas lannister adorei comprar pela I...</td>\n",
       "      <td>2018-03-01 00:00:00</td>\n",
       "      <td>2018-03-02 10:26:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          review_id                          order_id  \\\n",
       "0  7bc2406110b926393aa56f80a40eba40  73fc7af87114b39712e6da79b0a377eb   \n",
       "1  80e641a11e56f04c1ad469d5645fdfde  a548910a1c6147796b98fdf73dbeba33   \n",
       "2  228ce5500dc1d8e020d8d1322874b6f0  f9e4b658b201a9f2ecdecbb34bed034b   \n",
       "3  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
       "4  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
       "\n",
       "   review_score review_comment_title  \\\n",
       "0             4                  NaN   \n",
       "1             5                  NaN   \n",
       "2             5                  NaN   \n",
       "3             5                  NaN   \n",
       "4             5                  NaN   \n",
       "\n",
       "                              review_comment_message review_creation_date  \\\n",
       "0                                                NaN  2018-01-18 00:00:00   \n",
       "1                                                NaN  2018-03-10 00:00:00   \n",
       "2                                                NaN  2018-02-17 00:00:00   \n",
       "3              Recebi bem antes do prazo estipulado.  2017-04-21 00:00:00   \n",
       "4  Parabéns lojas lannister adorei comprar pela I...  2018-03-01 00:00:00   \n",
       "\n",
       "  review_answer_timestamp  \n",
       "0     2018-01-18 21:46:59  \n",
       "1     2018-03-11 03:05:13  \n",
       "2     2018-02-18 14:36:24  \n",
       "3     2017-04-21 22:02:06  \n",
       "4     2018-03-02 10:26:53  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'data/olist_order_reviews_dataset.csv')\n",
    "\n",
    "nan_counts = data.isnull().sum()\n",
    "\n",
    "print(nan_counts)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6ec7e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:19:06.636859Z",
     "iopub.status.busy": "2023-03-28T00:19:06.636859Z",
     "iopub.status.idle": "2023-03-28T00:19:06.977854Z",
     "shell.execute_reply": "2023-03-28T00:19:06.976854Z",
     "shell.execute_reply.started": "2023-03-28T00:19:06.636859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_score              0\n",
      "review_comment_message    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "review_score              0\n",
       "review_comment_message    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(data.columns.difference(['review_score','review_comment_message']), 1, inplace=True)\n",
    "data_drop_nans = data.dropna(subset=['review_comment_message'])\n",
    "nan_counts = data_drop_nans.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f74697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:17:37.770070Z",
     "iopub.status.busy": "2023-03-28T00:17:37.769069Z",
     "iopub.status.idle": "2023-03-28T00:17:38.107069Z",
     "shell.execute_reply": "2023-03-28T00:17:38.106070Z",
     "shell.execute_reply.started": "2023-03-28T00:17:37.770070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id                      0\n",
      "order_id                       0\n",
      "review_score                   0\n",
      "review_comment_title       31138\n",
      "review_comment_message         0\n",
      "review_creation_date           0\n",
      "review_answer_timestamp        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_drop_nans = data.dropna(subset=['review_comment_message'])\n",
    "nan_counts = data_drop_nans.isnull().sum()\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf55bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data(data, column_text='review_comment_message', \n",
    "#                column_score='review_score', \n",
    "#                points_cut = [0, 2, 5], \n",
    "#                classes = [0, 1]):\n",
    "\n",
    "#     df_bin = data\n",
    "#     df_bin = df_bin.dropna(subset=[column_text])\n",
    "#     df_bin['label'] = pd.cut(df_bin[column_score], bins=points_cut, labels=classes)\n",
    "#     df_bin = df_bin.rename(columns={column_text: 'text'})\n",
    "#     df_bin = df_bin[['text','label']]\n",
    "\n",
    "#     df_cat = data\n",
    "#     df_cat = df_cat.dropna(subset=[column_text])\n",
    "#     df_cat = df_cat.rename(columns={column_text: 'text' , column_score: 'label'})\n",
    "#     df_cat = df_cat[['text','label']]\n",
    "#     return df_bin ,df_cat\n",
    "\n",
    "# data_bin , data_cat = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=data_cat['label'], palette='winter')\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.title('Rating vs No of reviews')\n",
    "\n",
    "# Add count labels to each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'),\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha = 'center', va = 'center',\n",
    "                xytext = (0, 9),\n",
    "                textcoords = 'offset points',\n",
    "                fontsize=8)\n",
    "ax.set_ylim(0, 23000)  # Set the y-axis limit to 0 and 5000\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e587c06",
   "metadata": {},
   "source": [
    "# WORDCLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('portuguese')\n",
    "wordcloud = WordCloud(stopwords=stop_words,\n",
    "                      background_color=\"black\",\n",
    "                      width=1600, height=800).generate(' '.join(data_bin[\"text\"]))\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.set_axis_off()\n",
    "plt.imshow(wordcloud);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de3fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(data, split_train=TRAIN_SPLIT, random_state=RANDOM_STATE):\n",
    "    df_train = data.sample(frac = split_train, random_state = random_state)\n",
    "    df_test = data.drop(df_train.index)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for index, train in df_train.iterrows():\n",
    "        X_train.append(str(train['text']))\n",
    "        y_train.append(train['label'])\n",
    "\n",
    "    for index, test in df_test.iterrows():\n",
    "        X_test.append(str(test['text']))\n",
    "        y_test.append(test['label'])\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test) \n",
    "\n",
    "    return X_train , y_train , X_test , y_test\n",
    "\n",
    "X_train , y_train , X_test , y_test = split_test_train(data_bin, split_train=TRAIN_SPLIT, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token=OOV_TOK)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "def preprocess(X_train, X_test, max_length, vocab_size, trunc_type='post', oov_tok = \"<OOV>\"):\n",
    "\n",
    "    training_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train_padded = pad_sequences(training_sequences,maxlen=max_length, truncating=trunc_type)\n",
    "    \n",
    "    testing_sequences = tokencalizer.texts_to_sequences(X_test)\n",
    "    X_test_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
    "    \n",
    "    return X_train_padded, X_test_padded\n",
    "\n",
    "X_train, X_test = preprocess(X_train, X_test, MAX_LENGTH, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f51de1",
   "metadata": {},
   "source": [
    "# Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf222fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cm=confusion_matrix(y_test, lr_preds )\n",
    "nb_cm=confusion_matrix(y_test, nb_preds)\n",
    "svm_cm=confusion_matrix(y_test, sv_preds)\n",
    "nn_cm= confusion_matrix(y_test,y_preds)\n",
    "rf_cm = confusion_matrix(y_test,rf_preds)\n",
    "xg_cm = confusion_matrix(y_test,xg_preds)\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.suptitle(\"Confusion Matrices\",fontsize=24)\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Neural Network\")\n",
    "sns.heatmap(nn_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Naive Bayes\")\n",
    "sns.heatmap(nb_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.title(\"Support Vector Machine (SVM)\")\n",
    "sns.heatmap(svm_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(\"Logistic Regression\")\n",
    "sns.heatmap(lr_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(\"Random Forest Classifier\")\n",
    "sns.heatmap(rf_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(\"XGBosst Classifier\")\n",
    "sns.heatmap(xg_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e2ccc",
   "metadata": {},
   "source": [
    "# ROC-AUC Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a8947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "pred_lr = lr.predict_proba(X_test)[:,1]\n",
    "fpr_lr,tpr_lr,_ = roc_curve(y_test,pred_lr)\n",
    "roc_auc_lr = auc(fpr_lr,tpr_lr)\n",
    "\n",
    "pred_nb = nb.predict_proba(X_test)[:,1]\n",
    "fpr_nb,tpr_nb,_ = roc_curve(y_test,pred_nb)\n",
    "roc_auc_nb = auc(fpr_nb,tpr_nb)\n",
    "\n",
    "pred_svm = svm.decision_function(X_test)\n",
    "fpr_svm,tpr_svm,_ = roc_curve(y_test,pred_svm)\n",
    "roc_auc_svm = auc(fpr_svm,tpr_svm)\n",
    "\n",
    "fpr_nn, tpr_nn, _  = roc_curve(y_test, y_preds_prob)\n",
    "roc_auc_nn = auc(fpr_nn,tpr_nn)\n",
    "\n",
    "pred_rf = rfc.predict_proba(X_test)[:,1]\n",
    "fpr_rf,tpr_rf,_ = roc_curve(y_test,pred_rf)\n",
    "roc_auc_rf = auc(fpr_rf,tpr_rf)\n",
    "\n",
    "pred_xg = xgb.predict_proba(X_test)[:,1]\n",
    "fpr_xg,tpr_xg,_ = roc_curve(y_test,pred_xg)\n",
    "roc_auc_xg = auc(fpr_xg,tpr_xg)\n",
    "\n",
    "f, axes = plt.subplots(2,3,figsize=(20,15))\n",
    "axes[0,0].plot(fpr_nn, tpr_nn, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nn))\n",
    "axes[0,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[0,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\n",
    "axes[0,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'LSTM')\n",
    "axes[0,0].legend(loc='lower right', fontsize=13);\n",
    "\n",
    "\n",
    "axes[0,1].plot(fpr_nb, tpr_nb, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nb))\n",
    "axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[0,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\n",
    "axes[0,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Naive Bayes')\n",
    "axes[0,1].legend(loc='lower right', fontsize=13)\n",
    "\n",
    "axes[0,2].plot(fpr_svm, tpr_svm, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_svm))\n",
    "axes[0,2].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[0,2].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\n",
    "axes[0,2].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Support Vector Machine')\n",
    "axes[0,2].legend(loc='lower right', fontsize=13)\n",
    "\n",
    "axes[1,0].plot(fpr_lr, tpr_lr, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_lr))\n",
    "axes[1,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[1,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\n",
    "axes[1,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Logistic Regression')\n",
    "axes[1,0].legend(loc='lower right', fontsize=13)\n",
    "\n",
    "\n",
    "axes[1,1].plot(fpr_rf, tpr_rf, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_rf))\n",
    "axes[1,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[1,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\n",
    "axes[1,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Random Forest Classifier')\n",
    "axes[1,1].legend(loc='lower right', fontsize=13);\n",
    "\n",
    "\n",
    "axes[1,2].plot(fpr_xg, tpr_xg, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_xg))\n",
    "axes[1,2].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[1,2].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\n",
    "axes[1,2].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'XGB Classifier')\n",
    "axes[1,2].legend(loc='lower right', fontsize=13);\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
